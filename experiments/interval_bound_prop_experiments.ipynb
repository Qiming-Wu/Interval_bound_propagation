{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoK0sWEypQ-s"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Osqh5JlNpQ-2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from utils import epoch, epoch_robust_bound, epoch_calculate_robust_err, Flatten, generate_kappa_schedule_MNIST, generate_epsilon_schedule_MNIST\n",
    "from utils import bound_propagation, new_epoch_robust_bound, epoch_robust_bound\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jMEAJ22NpQ-9",
    "outputId": "57aee563-2fa9-4ec6-b92e-665334f0826b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xed682ed950>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "dataset_path = './cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root=dataset_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_mean = trainset.train_data.mean(axis=(0,1,2))/255  # [0.49139968  0.48215841  0.44653091]\n",
    "train_std = trainset.train_data.std(axis=(0,1,2))/255  # [0.24703223  0.24348513  0.26158784]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(\n",
    "    root=dataset_path, train=True, download=True,\n",
    "    transform=transform_train),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root=dataset_path, train=False, download=True,\n",
    "    transform=transform_test),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CJe2L46pQ_z"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_medium(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNN_medium, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=0, stride=1)\n",
    "        self.relu1 = nn.ReLU() \n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, padding=0, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=0, stride=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, 4, padding=0, stride=2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.flat = Flatten()\n",
    "        self.linear1 = nn.Linear(64*5*5, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.last_linear = nn.Linear(512, 10)                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden_activations = []\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        hidden_activations.append(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        hidden_activations.append(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        hidden_activations.append(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.flat(x)\n",
    "        hidden_activations.append(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu5(x)\n",
    "        hidden_activations.append(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu6(x)\n",
    "        hidden_activations.append(x)\n",
    "        \n",
    "        out = self.last_linear(x)\n",
    "        hidden_activations.append(out)\n",
    "        \n",
    "        return out, hidden_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = CNN_medium().to(device)\n",
    "model_robust = CNN_medium().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.load_state_dict(torch.load(\"CIFAR_trained_model.pth\"))\n",
    "model_robust.load_state_dict(torch.load(\"CIFAR_trained_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first train the model to reach >80% accuracy and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(45): \n",
    "    train_err, _ = epoch(train_loader, model, device, opt)\n",
    "    print (train_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"CIFAR_trained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"CIFAR_trained_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust.load_state_dict(torch.load(\"robust_layer_14_epoch_4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_robust_train(loader, model_ref, model_robust, epsilon, device, opt=None):\n",
    "    \n",
    "    total_loss_fit = 0\n",
    "    total_loss_spec = 0\n",
    "    \n",
    "    for X,y in loader:\n",
    "\n",
    "        loss_spec = []\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        real_values = []\n",
    "        \n",
    "        X,y = X.to(device), y.to(device)\n",
    "        \n",
    "        yp, hidden_activations = model_ref(X)\n",
    "        loss_fit = nn.CrossEntropyLoss(reduction=\"mean\")(yp,y) #calculate regular loss\n",
    "        \n",
    "        initial_bounds = (X-epsilon, X+epsilon)\n",
    "        bounds = bound_propagation(model_robust, initial_bounds, how_many_layers=14) #calculate bounds up to 14th layer\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[2][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[2][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[0])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[0].detach(), upper_bounds[0]) + nn.MSELoss()(real_values[0].detach(), lower_bounds[0]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[4][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[4][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[1])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[1].detach(), upper_bounds[1]) + nn.MSELoss()(real_values[1].detach(), lower_bounds[1]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[6][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[6][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[2])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[2].detach(), upper_bounds[2]) + nn.MSELoss()(real_values[2].detach(), lower_bounds[2]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[9][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[9][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[3])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[3].detach(), upper_bounds[3]) + nn.MSELoss()(real_values[3].detach(), lower_bounds[3]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[11][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[11][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[4])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[4].detach(), upper_bounds[4]) + nn.MSELoss()(real_values[4].detach(), lower_bounds[4]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[13][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[13][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[5])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[5].detach(), upper_bounds[5]) + nn.MSELoss()(real_values[5].detach(), lower_bounds[5]))\n",
    "        \n",
    "        lower_bounds.append(Flatten()(bounds[14][0])) #lower bounds \n",
    "        upper_bounds.append(Flatten()(bounds[14][1])) #upper bounds \n",
    "        real_values.append(Flatten()(hidden_activations[6])) #real activations \n",
    "        loss_spec.append(nn.MSELoss()(real_values[6].detach(), upper_bounds[6]) + nn.MSELoss()(real_values[6].detach(), lower_bounds[6]))\n",
    "                \n",
    "        \n",
    "        combined_loss = loss_spec[0] + loss_spec[1] + loss_spec[2] + loss_spec[3] + loss_spec[4] + loss_spec[5] + loss_spec[6]                                           \n",
    "        #combined_loss = loss_fit + loss_spec\n",
    "        \n",
    "        total_loss_fit += loss_fit.item() * X.shape[0]\n",
    "        total_loss_spec += loss_spec[6].item() * X.shape[0]\n",
    "        \n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "    return total_loss_fit/len(loader.dataset), total_loss_spec/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust.load_state_dict(torch.load(\"robust_layer_14_loss_63.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss_fit: 0.41829184756428     Loss_spec: 11915808678056.191\n",
      "Epoch 1: Loss_fit: 0.41829185001552105     Loss_spec: 10648875179.946\n",
      "Epoch 2: Loss_fit: 0.41829185144603254     Loss_spec: 2891487214.9535\n",
      "Epoch 3: Loss_fit: 0.41829184702038763     Loss_spec: 1053530492.9274563\n",
      "Epoch 4: Loss_fit: 0.4182918484508991     Loss_spec: 460019156.77685696\n",
      "Epoch 5: Loss_fit: 0.41829184933006763     Loss_spec: 221333956.88957378\n",
      "Epoch 6: Loss_fit: 0.4182918483838439     Loss_spec: 107087007.41236605\n",
      "Epoch 7: Loss_fit: 0.4182918471172452     Loss_spec: 51570353.14583178\n",
      "Epoch 8: Loss_fit: 0.4182918494567275     Loss_spec: 24586724.287693422\n",
      "Epoch 9: Loss_fit: 0.4182918496504426     Loss_spec: 11696254.194039695\n",
      "Epoch 10: Loss_fit: 0.4182918484434485     Loss_spec: 5715137.133328239\n",
      "Epoch 11: Loss_fit: 0.41829184725880625     Loss_spec: 2752379.4642795636\n",
      "Epoch 12: Loss_fit: 0.41829184879362585     Loss_spec: 1323863.570231842\n",
      "Epoch 13: Loss_fit: 0.41829184836149214     Loss_spec: 635488.0026461639\n"
     ]
    }
   ],
   "source": [
    "epsilon = 8/255\n",
    "opt = optim.Adam(model_robust.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(50): \n",
    "\n",
    "    \n",
    "    loss_fit, loss_spec = epoch_robust_train(train_loader, model_ref, model_robust, epsilon, device, opt)\n",
    "    #loss_fit, loss_spec = new_epoch_robust_bound(train_loader, model_robust, epsilon, device, opt) \n",
    "    print (f'Epoch {t}: Loss_fit: {loss_fit}     Loss_spec: {loss_spec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_robust.state_dict(), \"robust_bigger_epsilon_epoch_100.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 8/255\n",
    "how_many_layers = 14\n",
    "robust_err = epoch_calculate_robust_err (test_loader, model_robust, epsilon, how_many_layers, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (robust_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err, _ = epoch(test_loader, model_robust, device)\n",
    "print (f'Test error: {test_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBRQTG-zpQ_3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1649
    },
    "colab_type": "code",
    "id": "QgFHqVWQpQ_4",
    "outputId": "5c3db75b-72b4-4c0a-be92-26626a8be652"
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(model_cnn_medium.parameters(), lr=1e-3)\n",
    "\n",
    "EPSILON = 0.1\n",
    "EPSILON_TRAIN = 0.2\n",
    "epsilon_schedule = generate_epsilon_schedule_MNIST(EPSILON_TRAIN)\n",
    "kappa_schedule = generate_kappa_schedule_MNIST()\n",
    "batch_counter = 0\n",
    "\n",
    "print(\"Epoch   \", \"Combined Loss\", \"Test Err\", \"Test Robust Err\", sep=\"\\t\")\n",
    "\n",
    "for t in range(100):\n",
    "    _, combined_loss = epoch_robust_bound(train_loader, model_cnn_medium, epsilon_schedule, device, kappa_schedule, batch_counter, opt)\n",
    "    \n",
    "    # check loss and accuracy on test set\n",
    "    test_err, _ = epoch(test_loader, model_cnn_medium, device)\n",
    "    robust_err = epoch_calculate_robust_err(test_loader, model_cnn_medium, EPSILON, device)\n",
    "    \n",
    "    batch_counter += 600\n",
    "    \n",
    "    if t == 24:  #decrease learning rate after 25 epochs\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-4\n",
    "    \n",
    "    if t == 40:  #decrease learning rate after 41 epochs\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-5\n",
    "    \n",
    "    print(*(\"{:.6f}\".format(i) for i in (t, combined_loss, test_err, robust_err)), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIsnzpCfpQ_8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "interval_bound_prop.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
